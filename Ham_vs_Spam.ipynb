{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham vs Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ham-vs-Spam](https://raw.githubusercontent.com/sik-flow/nlp_text_classification/master/Pics/span-vs-ham.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a classifier to determine whether a message is ham or spam.  Data set is from the UCI Machine Learning Repository and can be found [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).\n",
    "\n",
    "First lets read in the data and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the Data\n",
    "\n",
    "data_source = 'https://raw.githubusercontent.com/sik-flow/nlp_text_classification/master/Data/SMSSpamCollection'\n",
    "df = pd.read_csv(data_source, delimiter= '\\t', header=None)\n",
    "df.columns = ['ham_spam', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"500\" height=\"500\" src=\"https://raw.githubusercontent.com/sik-flow/nlp_text_classification/master/Pics/dfhead1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first take a look at the most common words in ham messages and the most common messages in ham messages to see if there are any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words in ham messages:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "i      2181\n",
       "you    1669\n",
       "to     1552\n",
       "the    1125\n",
       "a      1058\n",
       "u       881\n",
       "and     846\n",
       "in      790\n",
       "my      745\n",
       "is      717\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('10 most common words in ham messages:')\n",
    "pd.Series(' '.join(df[df['ham_spam'] == 'ham'].text).lower().split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words in spam messages:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "to      685\n",
       "a       375\n",
       "call    342\n",
       "your    263\n",
       "you     252\n",
       "the     204\n",
       "for     202\n",
       "or      188\n",
       "free    180\n",
       "2       169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('10 most common words in spam messages:')\n",
    "pd.Series(' '.join(df[df['ham_spam'] == 'spam'].text).lower().split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, for ham messages the most common words are \"I\", \"you\", \"to\", and \"the\".  These words are known as [stop words](https://en.wikipedia.org/wiki/Stop_words).  Stop words are common words that are filtered out when doing Natural Language Processing. \n",
    "\n",
    "For spam messages we see that we have some stop words \"to\", \"a\", \"you\", \"the\", and it also has some words that will have some importance suchas as \"call\" and \"free\".  \n",
    "\n",
    "To start, we are going to leave the stop words in and see the results.  That way we can compare the effect of removing stop words. \n",
    "\n",
    "I'm now going to take the 10 most common words from ham and spam messages and make those columns with a binary format if that specific message has that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most common words in ham messages \n",
    "ham_words = pd.Series(' '.join(df[df['ham_spam'] == 'ham'].text).lower().split()).value_counts()[:10] \\\n",
    "    .index.values.tolist()\n",
    "    \n",
    "# Most common words in spam messages \n",
    "spam_words = pd.Series(' '.join(df[df['ham_spam'] == 'spam'].text).lower().split()).value_counts()[:10] \\\n",
    "    .index.values.tolist()\n",
    "    \n",
    "# Most common words of both messages\n",
    "common_words = ham_words + spam_words\n",
    "\n",
    "# Remove duplicate words\n",
    "common_words = list(set(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop to make columns and check if each word in common_words is in the message \n",
    "\n",
    "for word in common_words:\n",
    "    df[word] = df['text'].str.contains(\" \" + word + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"900\" height=\"900\" src=\"https://raw.githubusercontent.com/sik-flow/nlp_text_classification/master/Pics/dfhead2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our have data, so we are now going to use [Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html).  We use Bernoulli Naive Bayes because we are trying to predict a binary result (ham or spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89501076812634606"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Grab Data\n",
    "X = df[common_words]\n",
    "y = df['ham_spam']\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ended up with an accuracy of almost 90%, we're done right?\n",
    "\n",
    "![Not so fast, my friend](https://raw.githubusercontent.com/sik-flow/nlp_text_classification/master/Pics/corso.png)\n",
    "\n",
    "We need to see if you ham_spam column is unbalanced, if 99% of the messages are ham getting almost 90% is not very impressive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: ham_spam, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ham_spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8659368269921034"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['ham_spam'] == 'ham']) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ham_spam column is unbalanced.  If we would have predicted every message to be a ham message, we would have ended up with an accuracy of almost 87%.  So, getting 89% accuracy isn't so great anymore.\n",
    "\n",
    "Lets look at the confusion matrix to see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4699,  126],\n",
       "       [ 459,  288]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = clf.predict(X)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We correctly predicted 4,699 messages that were ham as ham, we incorrectly predicted 126 messages as spam that were ham, we incorrectly predicted 459 messages that were spam as ham, and correctly predicted 288 messages as spam that were spam.\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "Lets remove stop words and see if that helps out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stop = df[['ham_spam', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'seem', 'who', 'hereupon', 'con', 'is', 'towards', 'un', 'being', 'whenever', 'well', 'everywhere', 'myself', 'third', 'fill', 'above', 'be', 'beyond', 'etc', 'few', 'its', 'part', 'all', 'anything', 'almost', 'herein', 'made', 'however', 'such', 'hence', 'fifty', 'nine', 'serious', 'them', 'except', 'own', 'between', 'empty', 'seeming', 'what', 'among', 'became', 'give', 'hasnt', 'everyone', 'hereafter', 'same', 'cannot', 'so', 'some', 'this', 'next', 'our', 'though', 'seemed', 'ours', 'it', 're', 'system', 'her', 'again', 'might', 'must', 'my', 'thru', 'about', 'whole', 'his', 'thereafter', 'there', 'can', 'may', 'through', 'for', 'someone', 'whither', 'cant', 'amount', 'how', 'last', 'mine', 'off', 'these', 'herself', 'against', 'formerly', 'via', 'whereas', 'thus', 'that', 'whom', 'enough', 'from', 'perhaps', 'below', 'could', 'take', 'then', 'we', 'amoungst', 'sometimes', 'eleven', 'she', 'nor', 'sincere', 'to', 'whatever', 'or', 'yours', 'due', 'mostly', 'in', 'see', 'move', 'whereby', 'bottom', 'twenty', 'behind', 'beside', 'should', 'namely', 'us', 'whoever', 'further', 'around', 'no', 'after', 'always', 'sixty', 'since', 'wherein', 'others', 'than', 'less', 'latter', 'please', 'becoming', 'forty', 'front', 'if', 'but', 'every', 'another', 'would', 'him', 'whether', 'cry', 'else', 'into', 'rather', 'seems', 'along', 'already', 'meanwhile', 'interest', 'noone', 'a', 'thereupon', 'me', 'with', 'sometime', 'alone', 'when', 'out', 'four', 'still', 'besides', 'once', 'name', 'two', 'many', 'anyway', 'whereafter', 'afterwards', 'even', 'nowhere', 'because', 'get', 'detail', 'eg', 'nevertheless', 'anyone', 'more', 'and', 'five', 'full', 'will', 'ltd', 'down', 'also', 'co', 'too', 'where', 'within', 'couldnt', 'any', 'by', 'hers', 'onto', 'have', 'per', 'ten', 'often', 'before', 'therefore', 'side', 'do', 'very', 'never', 'at', 'each', 'other', 'much', 'ie', 'find', 'those', 'twelve', 'done', 'put', 'throughout', 'toward', 'why', 'your', 'were', 'i', 'keep', 'de', 'six', 'elsewhere', 'only', 'not', 'thence', 'upon', 'am', 'call', 'an', 'he', 'hereby', 'yourself', 'several', 'anyhow', 'go', 'thick', 'yourselves', 'become', 'becomes', 'show', 'here', 'nobody', 'themselves', 'until', 'wherever', 'three', 'whence', 'most', 'under', 'both', 'up', 'indeed', 'has', 'found', 'together', 'across', 'therein', 'without', 'was', 'describe', 'eight', 'fifteen', 'nothing', 'something', 'thin', 'which', 'thereby', 'whose', 'former', 'amongst', 'ever', 'are', 'anywhere', 'latterly', 'you', 'fire', 'they', 'itself', 'otherwise', 'moreover', 'ourselves', 'on', 'top', 'bill', 'mill', 'over', 'either', 'inc', 'their', 'beforehand', 'himself', 'now', 'whereupon', 'during', 'least', 'one', 'somewhere', 'been', 'back', 'first', 'yet', 'everything', 'as', 'somehow', 'although', 'had', 'hundred', 'while', 'neither', 'none', 'of', 'the'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the list of stop words in sklearn.  There is not a \"definitive\" list of stop words, you will see that NLTK has a slightly different list.  \n",
    "\n",
    "Lets remove stop words now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make everything lowercase\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "df[\"text\"] = df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Remove stop words\n",
    "stop = stop_words.ENGLISH_STOP_WORDS\n",
    "df_stop['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split()if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that stop words are removed lets see what the most common words are for ham and spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most common words in ham messages:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u        985\n",
       "im       460\n",
       "2        309\n",
       "just     290\n",
       "dont     276\n",
       "ltgt     276\n",
       "ok       273\n",
       "ur       246\n",
       "ill      242\n",
       "know     232\n",
       "got      232\n",
       "like     231\n",
       "come     227\n",
       "good     224\n",
       "love     190\n",
       "time     189\n",
       "day      188\n",
       "4        174\n",
       "Ã¼        169\n",
       "going    167\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('20 most common words in ham messages:')\n",
    "pd.Series(' '.join(df_stop[df_stop['ham_spam'] == 'ham']['text_without_stopwords']).lower().split()).value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most common words in spam messages:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "free      216\n",
       "2         173\n",
       "txt       150\n",
       "u         147\n",
       "ur        144\n",
       "mobile    123\n",
       "text      120\n",
       "4         119\n",
       "stop      115\n",
       "claim     113\n",
       "reply     101\n",
       "prize      92\n",
       "just       78\n",
       "won        73\n",
       "new        69\n",
       "send       68\n",
       "nokia      65\n",
       "urgent     63\n",
       "cash       62\n",
       "win        60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('20 most common words in spam messages:')\n",
    "pd.Series(' '.join(df_stop[df_stop['ham_spam'] == 'spam']['text_without_stopwords']).lower().split()).value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most common words for ham messages we can see trends - \"u\", \"im\", \"i'll\".  Spam messages also have trends such as \"free\", \"text\", \"txt\", \"mobile\", \"claim\".  \n",
    "\n",
    "Lets make a new data set with these words and see if we can improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most common words in ham messages \n",
    "ham_ns_words = pd.Series(' '.join(df_stop[df_stop['ham_spam'] == 'ham']['text_without_stopwords']).lower().split()).value_counts()[:20].index.values.tolist() \n",
    "    \n",
    "# Most common words in spam messages \n",
    "spam_ns_words = pd.Series(' '.join(df_stop[df_stop['ham_spam'] == 'spam']['text_without_stopwords']).lower().split()).value_counts()[:20].index.values.tolist() \n",
    "    \n",
    "# Most common words of both messages\n",
    "common_ns_words = ham_ns_words + spam_ns_words\n",
    "\n",
    "# Remove duplicate words\n",
    "common_ns_words = list(set(common_ns_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop to make columns and check if each word in common_words is in the message \n",
    "\n",
    "for word in common_ns_words:\n",
    "    df_stop[word] = df_stop['text_without_stopwords'].str.contains(\" \" + word + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94669777458722182"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_stop[common_ns_words]\n",
    "y = df_stop['ham_spam']\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4771,   54],\n",
       "       [ 243,  504]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with an accuracy of almost 95% and our confusion matrix looks better as well.  Originally, we had 126 messages that were classified as spam that were ham, now we only have 54 messages that were classified as spam that were ham.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Next we'll see if we can improve our score even more using TF-IDF.  TF-IDF stands for Term Frequency-Inverse Document Frequency. Term frequency shows how often a word appears in a document.  While inverse document frequency downscales words that appear a lot across documents.  In short words with a high TF-IDF score are words that appear frequently in the document and provide the most information about that specific document.  \n",
    "\n",
    "Lets convert our data into a TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\"english\")\n",
    "\n",
    "tf_idf = tfidf.fit_transform(df_stop['text_without_stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98546302943287867"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(tf_idf, y)\n",
    "clf.score(tf_idf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4823,    2],\n",
       "       [  79,  668]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(tf_idf)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with an accuracy of almost 99% and our confusion matrix looks better as well.  Also, we ended with only 2 messages that we classified as spam that was actually ham.  This is very important since misclassifying a ham message means that the user more than likely will not see the message and they need to see that message.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We learned the following tools/techniques:\n",
    "\n",
    "1. What stop words are\n",
    "2. Basic Text Classification using most common words\n",
    "3. Basic Text Classification using TF-IDF\n",
    "\n",
    "Use these NLP skills for good. Try classifying IMDB reviews using data [here](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "![Use NLP for good](https://raw.githubusercontent.com/sik-flow/nlp_text_classification/master/Pics/drevil1.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
